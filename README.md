# 🧠 llm-zero-to-hero
A hands-on learning repo documenting my journey to mastering LLMs from first principles — inspired by Andrej Karpathy’s *Neural Networks: Zero to Hero* and *makemore* series.

This project includes notebooks and notes where I explore core concepts such as backpropagation, language modeling, tokenization, embeddings, and neural network internals — all built from scratch using Python.

---

## 📚 Projects

### 🔹 1. `micrograd` – Reverse-Mode Autodiff from Scratch  
📄 [`micrograd.ipynb`](./micrograd.ipynb)  
▶️ Based on Karpathy’s [Zero to Hero video #1](https://www.youtube.com/watch?v=VMj-3S1tku0)  
💡 Learnings:
- Manual implementation of forward and backward pass
- Scalar expression graphs and gradient tracking
- Gained deep intuition for autograd systems like PyTorch

---

### 🔹 2. `makemore` – Bigram Language Model  
📄 [`makemore.ipynb`](./makemore.ipynb)  
▶️ Based on [makemore video #1](https://www.youtube.com/watch?v=PaCmpygFfXo)  
💡 Learnings:
- Building a character-level bigram model
- Constructing transition probability matrices
- Generating names with sampling from learned statistics

---

## 🔄 What's Next
I'm currently diving deeper into:
- Strengthening Python fundamentals
- Core packages: **NumPy**, **Pandas**, and **PyTorch**
- Building neural versions of the bigram model

Follow this repo or connect on [LinkedIn](https://www.linkedin.com/in/naveen-kumar-ba4a1215/) for updates as I go from zero to building custom LLMs.

---

## 🧭 Roadmap Preview
- ✅ Week 1–2: Linear Algebra, Calculus, Probability
- ✅ Week 3–4: Python Core + Micrograd
- ✅ Week 5: Bigram Language Model
- 🔜 Week 6+: NumPy, PyTorch, Transformers

---

## 💬 Feedback Welcome!
Feel free to open issues or share suggestions — learning is better when it’s collaborative!
