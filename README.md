# ğŸ§  llm-zero-to-hero
A hands-on learning repo documenting my journey to mastering LLMs from first principles â€” inspired by Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* and *makemore* series.

This project includes notebooks and notes where I explore core concepts such as backpropagation, language modeling, tokenization, embeddings, and neural network internals â€” all built from scratch using Python.

---

## ğŸ“š Projects

### ğŸ”¹ 1. `micrograd` â€“ Reverse-Mode Autodiff from Scratch  
ğŸ“„ [`micrograd.ipynb`](./micrograd.ipynb)  
â–¶ï¸ Based on Karpathyâ€™s [Zero to Hero video #1](https://www.youtube.com/watch?v=VMj-3S1tku0)  
ğŸ’¡ Learnings:
- Manual implementation of forward and backward pass
- Scalar expression graphs and gradient tracking
- Gained deep intuition for autograd systems like PyTorch

---

### ğŸ”¹ 2. `makemore` â€“ Bigram Language Model  
ğŸ“„ [`makemore.ipynb`](./makemore.ipynb)  
â–¶ï¸ Based on [makemore video #1](https://www.youtube.com/watch?v=PaCmpygFfXo)  
ğŸ’¡ Learnings:
- Building a character-level bigram model
- Constructing transition probability matrices
- Generating names with sampling from learned statistics

---

## ğŸ”„ What's Next
I'm currently diving deeper into:
- Strengthening Python fundamentals
- Core packages: **NumPy**, **Pandas**, and **PyTorch**
- Building neural versions of the bigram model

Follow this repo or connect on [LinkedIn](https://www.linkedin.com/in/naveen-kumar-ba4a1215/) for updates as I go from zero to building custom LLMs.

---

## ğŸ§­ Roadmap Preview
- âœ… Week 1â€“2: Linear Algebra, Calculus, Probability
- âœ… Week 3â€“4: Python Core + Micrograd
- âœ… Week 5: Bigram Language Model
- ğŸ”œ Week 6+: NumPy, PyTorch, Transformers

---

## ğŸ’¬ Feedback Welcome!
Feel free to open issues or share suggestions â€” learning is better when itâ€™s collaborative!
